{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNST_CIFAR10_AUG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chirag2saraiya/Session-16_DenseNet_TFRecord/blob/master/DNST_CIFAR10_AUG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K70hAckqg0EA",
        "colab_type": "code",
        "outputId": "1416c9c2-396e-4a4f-89ce-eb525b27689f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "!rm -rf \"/content/DeepLearning\"\n",
        "!git clone https://github.com/chirag2saraiya/DeepLearning.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepLearning'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 121 (delta 59), reused 0 (delta 0), pack-reused 0\n",
            "Receiving objects: 100% (121/121), 66.80 KiB | 3.93 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVIx_KIigxPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from DeepLearning import tfrecords\n",
        "from DeepLearning import earlystop_callback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNHw6luQg3gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsO_yGxcg5D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 256\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "l = 40\n",
        "num_filter = 12\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2CS-9fJ3j8m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "e3180bbe-bbec-490d-accf-a1eb43bbf19d"
      },
      "source": [
        "tfrecords.convert('/content/cifar10')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and extract.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/DeepLearning/tfrecords.py:27: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n",
            "Generating /content/cifar10/train.tfrecords\n",
            "WARNING:tensorflow:From /content/DeepLearning/tfrecords.py:60: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/DeepLearning/tfrecords.py:49: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Generating /content/cifar10/test.tfrecords\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-StJtxt-5yqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLQNxR5t73pN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "3291eece-7de7-42c6-9fed-4d42cb643d8d"
      },
      "source": [
        "x_train,y_train = tfrecords.Cifar10DataSet(\"/content/cifar10\").make_batch(batch_size)\n",
        "print(x_train)\n",
        "print(y_train)\n",
        "x_test,y_test = tfrecords.Cifar10DataSet(\"/content/cifar10\",\"test\",False).make_batch(batch_size)\n",
        "print(x_test)\n",
        "print(y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/DeepLearning/tfrecords.py:112: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/DeepLearning/tfrecords.py:115: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/DeepLearning/tfrecords.py:161: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/DeepLearning/tfrecords.py:162: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/DeepLearning/tfrecords.py:152: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "Tensor(\"IteratorGetNext:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
            "Tensor(\"IteratorGetNext:1\", shape=(?, 10), dtype=float32)\n",
            "Tensor(\"IteratorGetNext_1:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
            "Tensor(\"IteratorGetNext_1:1\", shape=(?, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee-sge5Kg7vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOP6IPsGhBwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RaKFpubhDIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anPCpQWhhGb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f91c4e74-ca8a-4f92-9023-02af34c71ca8"
      },
      "source": [
        "num_filter = 12\n",
        "dropout_rate = 0.2\n",
        "l = 12\n",
        "input = Input(tensor=x_train)\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kFh7pdxhNtT",
        "colab_type": "code",
        "outputId": "21af7b5e-2236-42e1-ec35-de9c14f31abd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 12)   324         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 12)   48          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 12)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 12)   1296        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 12)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 24)   0           conv2d_1[0][0]                   \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 24)   96          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 24)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 12)   2592        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 12)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 36)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 36)   144         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 36)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 12)   3888        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 12)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 48)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 48)   192         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 48)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 12)   5184        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 12)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 60)   0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 60)   240         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 60)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 12)   6480        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 12)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 72)   0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 72)   288         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 72)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 12)   7776        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 12)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 84)   0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 84)   336         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 84)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 12)   9072        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 12)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 96)   0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 96)   384         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 96)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 12)   10368       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 12)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 108)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 108)  432         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 108)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 12)   11664       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 12)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 120)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 120)  480         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 120)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 12)   12960       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 12)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 132)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 132)  528         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 132)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 12)   14256       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 12)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 144)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 144)  576         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 144)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 12)   15552       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 12)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 156)  0           concatenate_11[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 156)  624         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 156)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 6)    936         activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 6)    0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 6)    0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 6)    24          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 6)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 12)   648         activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 12)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 18)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 18)   72          concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 18)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 12)   1944        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 12)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 30)   0           concatenate_13[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 30)   120         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 30)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 12)   3240        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 16, 16, 12)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 42)   0           concatenate_14[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 42)   168         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 42)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 12)   4536        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 12)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 54)   0           concatenate_15[0][0]             \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 54)   216         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 54)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 12)   5832        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 12)   0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 66)   0           concatenate_16[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 66)   264         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 66)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 12)   7128        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 16, 16, 12)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 78)   0           concatenate_17[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 78)   312         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 78)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 12)   8424        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 12)   0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 90)   0           concatenate_18[0][0]             \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 90)   360         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 90)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 12)   9720        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 16, 16, 12)   0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 102)  0           concatenate_19[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 102)  408         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 102)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 12)   11016       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 12)   0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 114)  0           concatenate_20[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 114)  456         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 114)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 12)   12312       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16, 16, 12)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 126)  0           concatenate_21[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 126)  504         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 126)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 12)   13608       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 16, 16, 12)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 138)  0           concatenate_22[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 138)  552         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 138)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 12)   14904       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 16, 16, 12)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 150)  0           concatenate_23[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 150)  600         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 150)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 6)    900         activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 6)    0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 6)      0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 6)      24          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 6)      0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 12)     648         activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 8, 8, 12)     0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 18)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 18)     72          concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 18)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 12)     1944        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 8, 8, 12)     0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 30)     0           concatenate_25[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 30)     120         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 30)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 12)     3240        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 8, 8, 12)     0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 42)     0           concatenate_26[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 42)     168         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 42)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 12)     4536        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 8, 8, 12)     0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 54)     0           concatenate_27[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 54)     216         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 54)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 12)     5832        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 8, 8, 12)     0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 66)     0           concatenate_28[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 66)     264         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 66)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 12)     7128        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 8, 8, 12)     0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 78)     0           concatenate_29[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 78)     312         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 78)     0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 12)     8424        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 8, 8, 12)     0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 90)     0           concatenate_30[0][0]             \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 90)     360         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 90)     0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 12)     9720        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 8, 8, 12)     0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 102)    0           concatenate_31[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 102)    408         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 102)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 12)     11016       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 8, 8, 12)     0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 114)    0           concatenate_32[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 114)    456         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 114)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 12)     12312       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 8, 8, 12)     0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 126)    0           concatenate_33[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 126)    504         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 126)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 12)     13608       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 8, 8, 12)     0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 138)    0           concatenate_34[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 138)    552         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 138)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 12)     14904       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 8, 12)     0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 150)    0           concatenate_35[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 150)    600         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 150)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 6)      900         activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 8, 8, 6)      0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 4, 4, 6)      0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 6)      24          average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 6)      0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 12)     648         activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 4, 4, 12)     0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 18)     0           average_pooling2d_3[0][0]        \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 18)     72          concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 18)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 12)     1944        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 4, 4, 12)     0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 30)     0           concatenate_37[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 30)     120         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 30)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 12)     3240        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 4, 4, 12)     0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 42)     0           concatenate_38[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 42)     168         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 42)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 12)     4536        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 4, 4, 12)     0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 54)     0           concatenate_39[0][0]             \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 54)     216         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 54)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 12)     5832        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 4, 4, 12)     0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 66)     0           concatenate_40[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 66)     264         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 66)     0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 12)     7128        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 4, 4, 12)     0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 78)     0           concatenate_41[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 78)     312         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 78)     0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 12)     8424        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 4, 4, 12)     0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 90)     0           concatenate_42[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 90)     360         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 90)     0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 12)     9720        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 4, 4, 12)     0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 102)    0           concatenate_43[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 102)    408         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 102)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 12)     11016       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 4, 4, 12)     0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 114)    0           concatenate_44[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 114)    456         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 114)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 12)     12312       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 4, 4, 12)     0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 126)    0           concatenate_45[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 126)    504         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 126)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 12)     13608       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 4, 4, 12)     0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 138)    0           concatenate_46[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 138)    552         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 138)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 12)     14904       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 4, 4, 12)     0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 4, 4, 150)    0           concatenate_47[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 4, 150)    600         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 4, 150)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 2, 2, 150)    0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 600)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           6010        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 406,630\n",
            "Trainable params: 398,362\n",
            "Non-trainable params: 8,268\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4XOsW3ahSkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1e077c60-86b6-47af-c793-bcdb382e4b16"
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'],\n",
        "              target_tensors=[y_train])\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crhGk7kEhXAz",
        "colab_type": "code",
        "outputId": "57b4042c-5235-4dab-928a-e8ef9dbeb410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "filepath= \"/content/best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint,earlystop_callback.TerminateOnBaseline(monitor='val_acc', baseline=0.85)]\n",
        "\n",
        "\n",
        "model.fit(epochs=150,steps_per_epoch=100,verbose=1,validation_data=(x_test,y_test),validation_steps = 100,callbacks = callbacks_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/150\n",
            "100/100 [==============================] - 130s 1s/step - loss: 2.0094 - acc: 0.2471 - val_loss: 1.9055 - val_acc: 0.2994\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.29938, saving model to /content/best_model.hdf5\n",
            "Epoch 2/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.7041 - acc: 0.3609 - val_loss: 2.4564 - val_acc: 0.2938\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.29938\n",
            "Epoch 3/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.5666 - acc: 0.4147 - val_loss: 1.8796 - val_acc: 0.3957\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.29938 to 0.39574, saving model to /content/best_model.hdf5\n",
            "Epoch 4/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.4620 - acc: 0.4622 - val_loss: 1.8952 - val_acc: 0.3991\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.39574 to 0.39906, saving model to /content/best_model.hdf5\n",
            "Epoch 5/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.3799 - acc: 0.4896 - val_loss: 1.7296 - val_acc: 0.4296\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.39906 to 0.42957, saving model to /content/best_model.hdf5\n",
            "Epoch 6/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.3131 - acc: 0.5181 - val_loss: 1.4712 - val_acc: 0.4971\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.42957 to 0.49715, saving model to /content/best_model.hdf5\n",
            "Epoch 7/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.2467 - acc: 0.5455 - val_loss: 1.4687 - val_acc: 0.4954\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.49715\n",
            "Epoch 8/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.2058 - acc: 0.5626 - val_loss: 1.6363 - val_acc: 0.4584\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.49715\n",
            "Epoch 9/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.1506 - acc: 0.5842 - val_loss: 1.3492 - val_acc: 0.5384\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.49715 to 0.53836, saving model to /content/best_model.hdf5\n",
            "Epoch 10/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.1266 - acc: 0.5930 - val_loss: 1.2364 - val_acc: 0.5671\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.53836 to 0.56711, saving model to /content/best_model.hdf5\n",
            "Epoch 11/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.0921 - acc: 0.6034 - val_loss: 1.2477 - val_acc: 0.5668\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.56711\n",
            "Epoch 12/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.0501 - acc: 0.6211 - val_loss: 1.4906 - val_acc: 0.5324\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.56711\n",
            "Epoch 13/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.0390 - acc: 0.6226 - val_loss: 1.2595 - val_acc: 0.5819\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.56711 to 0.58188, saving model to /content/best_model.hdf5\n",
            "Epoch 14/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 1.0222 - acc: 0.6303 - val_loss: 1.0468 - val_acc: 0.6345\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.58188 to 0.63453, saving model to /content/best_model.hdf5\n",
            "Epoch 15/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.9831 - acc: 0.6457 - val_loss: 1.2452 - val_acc: 0.5863\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.63453\n",
            "Epoch 16/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.9727 - acc: 0.6486 - val_loss: 2.3178 - val_acc: 0.4587\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.63453\n",
            "Epoch 17/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.9544 - acc: 0.6528 - val_loss: 1.6366 - val_acc: 0.5470\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.63453\n",
            "Epoch 18/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.9389 - acc: 0.6594 - val_loss: 1.0665 - val_acc: 0.6464\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.63453 to 0.64637, saving model to /content/best_model.hdf5\n",
            "Epoch 19/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.9259 - acc: 0.6676 - val_loss: 1.4099 - val_acc: 0.5618\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.64637\n",
            "Epoch 20/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.9068 - acc: 0.6750 - val_loss: 1.0849 - val_acc: 0.6367\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.64637\n",
            "Epoch 21/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.8885 - acc: 0.6798 - val_loss: 1.3780 - val_acc: 0.5722\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.64637\n",
            "Epoch 22/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.8780 - acc: 0.6852 - val_loss: 1.4064 - val_acc: 0.6067\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.64637\n",
            "Epoch 23/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.8709 - acc: 0.6915 - val_loss: 1.1014 - val_acc: 0.6475\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.64637 to 0.64754, saving model to /content/best_model.hdf5\n",
            "Epoch 24/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.8599 - acc: 0.6903 - val_loss: 1.0835 - val_acc: 0.6491\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.64754 to 0.64914, saving model to /content/best_model.hdf5\n",
            "Epoch 25/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.8463 - acc: 0.6967 - val_loss: 1.6879 - val_acc: 0.5501\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.64914\n",
            "Epoch 26/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.8294 - acc: 0.7029 - val_loss: 1.2374 - val_acc: 0.6162\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.64914\n",
            "Epoch 27/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.8281 - acc: 0.7029 - val_loss: 0.9999 - val_acc: 0.6708\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.64914 to 0.67082, saving model to /content/best_model.hdf5\n",
            "Epoch 28/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.8166 - acc: 0.7093 - val_loss: 1.1429 - val_acc: 0.6460\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.67082\n",
            "Epoch 29/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.8153 - acc: 0.7077 - val_loss: 1.0008 - val_acc: 0.6740\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.67082 to 0.67398, saving model to /content/best_model.hdf5\n",
            "Epoch 30/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.7956 - acc: 0.7163 - val_loss: 1.2974 - val_acc: 0.6350\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.67398\n",
            "Epoch 31/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.7735 - acc: 0.7237 - val_loss: 0.9404 - val_acc: 0.6911\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.67398 to 0.69113, saving model to /content/best_model.hdf5\n",
            "Epoch 32/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.7873 - acc: 0.7217 - val_loss: 1.3345 - val_acc: 0.6048\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.69113\n",
            "Epoch 33/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.7640 - acc: 0.7283 - val_loss: 0.8374 - val_acc: 0.7114\n",
            "\n",
            "Epoch 00033: val_acc improved from 0.69113 to 0.71137, saving model to /content/best_model.hdf5\n",
            "Epoch 34/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.7562 - acc: 0.7341 - val_loss: 0.9723 - val_acc: 0.6786\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.71137\n",
            "Epoch 35/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.7556 - acc: 0.7293 - val_loss: 0.9903 - val_acc: 0.6950\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.71137\n",
            "Epoch 36/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.7464 - acc: 0.7297 - val_loss: 0.9452 - val_acc: 0.7009\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.71137\n",
            "Epoch 37/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.7280 - acc: 0.7426 - val_loss: 0.9362 - val_acc: 0.7050\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.71137\n",
            "Epoch 38/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.7360 - acc: 0.7370 - val_loss: 0.9512 - val_acc: 0.6966\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.71137\n",
            "Epoch 39/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.7249 - acc: 0.7429 - val_loss: 1.2946 - val_acc: 0.6409\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.71137\n",
            "Epoch 40/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.7116 - acc: 0.7458 - val_loss: 1.1053 - val_acc: 0.6687\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.71137\n",
            "Epoch 41/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.7072 - acc: 0.7478 - val_loss: 0.8485 - val_acc: 0.7268\n",
            "\n",
            "Epoch 00041: val_acc improved from 0.71137 to 0.72684, saving model to /content/best_model.hdf5\n",
            "Epoch 42/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.6946 - acc: 0.7543 - val_loss: 0.9210 - val_acc: 0.7071\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.72684\n",
            "Epoch 43/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.6940 - acc: 0.7523 - val_loss: 1.2157 - val_acc: 0.6526\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.72684\n",
            "Epoch 44/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.6972 - acc: 0.7520 - val_loss: 0.9089 - val_acc: 0.7096\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.72684\n",
            "Epoch 45/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.6815 - acc: 0.7593 - val_loss: 1.0570 - val_acc: 0.6906\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.72684\n",
            "Epoch 46/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.6685 - acc: 0.7636 - val_loss: 0.7193 - val_acc: 0.7643\n",
            "\n",
            "Epoch 00046: val_acc improved from 0.72684 to 0.76430, saving model to /content/best_model.hdf5\n",
            "Epoch 47/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.6802 - acc: 0.7577 - val_loss: 1.1907 - val_acc: 0.6655\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.76430\n",
            "Epoch 48/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.6656 - acc: 0.7637 - val_loss: 0.6841 - val_acc: 0.7694\n",
            "\n",
            "Epoch 00048: val_acc improved from 0.76430 to 0.76938, saving model to /content/best_model.hdf5\n",
            "Epoch 49/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.6506 - acc: 0.7675 - val_loss: 1.4617 - val_acc: 0.6315\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.76938\n",
            "Epoch 50/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.6597 - acc: 0.7679 - val_loss: 0.7813 - val_acc: 0.7436\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.76938\n",
            "Epoch 51/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.6352 - acc: 0.7766 - val_loss: 0.8859 - val_acc: 0.7088\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.76938\n",
            "Epoch 52/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.6452 - acc: 0.7741 - val_loss: 0.8752 - val_acc: 0.7347\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.76938\n",
            "Epoch 53/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.6409 - acc: 0.7721 - val_loss: 1.7211 - val_acc: 0.5995\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.76938\n",
            "Epoch 54/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.6340 - acc: 0.7773 - val_loss: 1.3919 - val_acc: 0.6375\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.76938\n",
            "Epoch 55/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.6159 - acc: 0.7841 - val_loss: 0.9083 - val_acc: 0.7223\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.76938\n",
            "Epoch 56/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.6258 - acc: 0.7770 - val_loss: 0.8500 - val_acc: 0.7347\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.76938\n",
            "Epoch 57/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.6130 - acc: 0.7839 - val_loss: 0.6658 - val_acc: 0.7746\n",
            "\n",
            "Epoch 00057: val_acc improved from 0.76938 to 0.77461, saving model to /content/best_model.hdf5\n",
            "Epoch 58/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.6006 - acc: 0.7860 - val_loss: 0.6957 - val_acc: 0.7715\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.77461\n",
            "Epoch 59/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.6137 - acc: 0.7835 - val_loss: 0.8971 - val_acc: 0.7328\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.77461\n",
            "Epoch 60/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5966 - acc: 0.7883 - val_loss: 0.6427 - val_acc: 0.7891\n",
            "\n",
            "Epoch 00060: val_acc improved from 0.77461 to 0.78914, saving model to /content/best_model.hdf5\n",
            "Epoch 61/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5976 - acc: 0.7886 - val_loss: 0.7800 - val_acc: 0.7562\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.78914\n",
            "Epoch 62/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5979 - acc: 0.7895 - val_loss: 0.7619 - val_acc: 0.7555\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.78914\n",
            "Epoch 63/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5879 - acc: 0.7927 - val_loss: 0.9316 - val_acc: 0.7280\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.78914\n",
            "Epoch 64/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5926 - acc: 0.7901 - val_loss: 0.7428 - val_acc: 0.7631\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.78914\n",
            "Epoch 65/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5775 - acc: 0.7989 - val_loss: 0.6717 - val_acc: 0.7853\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.78914\n",
            "Epoch 66/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5788 - acc: 0.7975 - val_loss: 1.1209 - val_acc: 0.6921\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.78914\n",
            "Epoch 67/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5725 - acc: 0.8008 - val_loss: 1.0611 - val_acc: 0.7214\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.78914\n",
            "Epoch 68/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5591 - acc: 0.8031 - val_loss: 0.7299 - val_acc: 0.7673\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.78914\n",
            "Epoch 69/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5684 - acc: 0.8000 - val_loss: 1.0323 - val_acc: 0.7251\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.78914\n",
            "Epoch 70/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5624 - acc: 0.8008 - val_loss: 0.6444 - val_acc: 0.7913\n",
            "\n",
            "Epoch 00070: val_acc improved from 0.78914 to 0.79125, saving model to /content/best_model.hdf5\n",
            "Epoch 71/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5660 - acc: 0.8007 - val_loss: 0.6334 - val_acc: 0.7973\n",
            "\n",
            "Epoch 00071: val_acc improved from 0.79125 to 0.79734, saving model to /content/best_model.hdf5\n",
            "Epoch 72/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5534 - acc: 0.8018 - val_loss: 0.9110 - val_acc: 0.7325\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.79734\n",
            "Epoch 73/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5430 - acc: 0.8115 - val_loss: 0.7972 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.79734\n",
            "Epoch 74/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5586 - acc: 0.8043 - val_loss: 0.5723 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00074: val_acc improved from 0.79734 to 0.81027, saving model to /content/best_model.hdf5\n",
            "Epoch 75/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5431 - acc: 0.8099 - val_loss: 0.6897 - val_acc: 0.7902\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.81027\n",
            "Epoch 76/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5475 - acc: 0.8109 - val_loss: 0.7218 - val_acc: 0.7861\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.81027\n",
            "Epoch 77/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5503 - acc: 0.8087 - val_loss: 0.6113 - val_acc: 0.8072\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.81027\n",
            "Epoch 78/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5418 - acc: 0.8070 - val_loss: 0.7609 - val_acc: 0.7811\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.81027\n",
            "Epoch 79/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5394 - acc: 0.8108 - val_loss: 0.6906 - val_acc: 0.7860\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.81027\n",
            "Epoch 80/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5350 - acc: 0.8102 - val_loss: 0.7401 - val_acc: 0.7736\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.81027\n",
            "Epoch 81/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5425 - acc: 0.8071 - val_loss: 0.5946 - val_acc: 0.8066\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.81027\n",
            "Epoch 82/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5206 - acc: 0.8158 - val_loss: 1.2656 - val_acc: 0.6775\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.81027\n",
            "Epoch 83/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5252 - acc: 0.8153 - val_loss: 0.6180 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.81027\n",
            "Epoch 84/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5342 - acc: 0.8145 - val_loss: 0.6354 - val_acc: 0.8012\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.81027\n",
            "Epoch 85/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5216 - acc: 0.8188 - val_loss: 0.6899 - val_acc: 0.7871\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.81027\n",
            "Epoch 86/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5148 - acc: 0.8189 - val_loss: 0.9274 - val_acc: 0.7451\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.81027\n",
            "Epoch 87/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5152 - acc: 0.8196 - val_loss: 0.6015 - val_acc: 0.8123\n",
            "\n",
            "Epoch 00087: val_acc improved from 0.81027 to 0.81227, saving model to /content/best_model.hdf5\n",
            "Epoch 88/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5151 - acc: 0.8189 - val_loss: 0.6364 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.81227\n",
            "Epoch 89/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.5142 - acc: 0.8174 - val_loss: 0.5852 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.81227\n",
            "Epoch 90/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4968 - acc: 0.8267 - val_loss: 0.8165 - val_acc: 0.7660\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.81227\n",
            "Epoch 91/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5073 - acc: 0.8211 - val_loss: 0.8202 - val_acc: 0.7704\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.81227\n",
            "Epoch 92/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5113 - acc: 0.8199 - val_loss: 0.6035 - val_acc: 0.8066\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.81227\n",
            "Epoch 93/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5057 - acc: 0.8225 - val_loss: 0.6207 - val_acc: 0.8124\n",
            "\n",
            "Epoch 00093: val_acc improved from 0.81227 to 0.81238, saving model to /content/best_model.hdf5\n",
            "Epoch 94/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4994 - acc: 0.8240 - val_loss: 0.6773 - val_acc: 0.7980\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.81238\n",
            "Epoch 95/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.5054 - acc: 0.8229 - val_loss: 0.5691 - val_acc: 0.8173\n",
            "\n",
            "Epoch 00095: val_acc improved from 0.81238 to 0.81727, saving model to /content/best_model.hdf5\n",
            "Epoch 96/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4901 - acc: 0.8265 - val_loss: 1.0466 - val_acc: 0.7182\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.81727\n",
            "Epoch 97/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4895 - acc: 0.8279 - val_loss: 1.2071 - val_acc: 0.6954\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.81727\n",
            "Epoch 98/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4895 - acc: 0.8261 - val_loss: 0.8101 - val_acc: 0.7645\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.81727\n",
            "Epoch 99/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4856 - acc: 0.8280 - val_loss: 0.6836 - val_acc: 0.7857\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.81727\n",
            "Epoch 100/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.4845 - acc: 0.8313 - val_loss: 0.6286 - val_acc: 0.8080\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.81727\n",
            "Epoch 101/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4845 - acc: 0.8312 - val_loss: 0.9230 - val_acc: 0.7543\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.81727\n",
            "Epoch 102/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4861 - acc: 0.8285 - val_loss: 0.6271 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.81727\n",
            "Epoch 103/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4723 - acc: 0.8357 - val_loss: 0.6782 - val_acc: 0.7985\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.81727\n",
            "Epoch 104/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4837 - acc: 0.8291 - val_loss: 0.6550 - val_acc: 0.7944\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.81727\n",
            "Epoch 105/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4696 - acc: 0.8358 - val_loss: 0.5768 - val_acc: 0.8201\n",
            "\n",
            "Epoch 00105: val_acc improved from 0.81727 to 0.82012, saving model to /content/best_model.hdf5\n",
            "Epoch 106/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4628 - acc: 0.8388 - val_loss: 0.4776 - val_acc: 0.8444\n",
            "\n",
            "Epoch 00106: val_acc improved from 0.82012 to 0.84441, saving model to /content/best_model.hdf5\n",
            "Epoch 107/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.4701 - acc: 0.8366 - val_loss: 0.6432 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.84441\n",
            "Epoch 108/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4718 - acc: 0.8330 - val_loss: 0.8434 - val_acc: 0.7712\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.84441\n",
            "Epoch 109/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.4679 - acc: 0.8366 - val_loss: 0.5506 - val_acc: 0.8216\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.84441\n",
            "Epoch 110/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.4602 - acc: 0.8387 - val_loss: 0.5048 - val_acc: 0.8412\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.84441\n",
            "Epoch 111/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4628 - acc: 0.8373 - val_loss: 0.5246 - val_acc: 0.8335\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.84441\n",
            "Epoch 112/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4575 - acc: 0.8392 - val_loss: 0.6224 - val_acc: 0.8111\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.84441\n",
            "Epoch 113/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4598 - acc: 0.8416 - val_loss: 0.6729 - val_acc: 0.7967\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.84441\n",
            "Epoch 114/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4497 - acc: 0.8411 - val_loss: 0.4636 - val_acc: 0.8472\n",
            "\n",
            "Epoch 00114: val_acc improved from 0.84441 to 0.84719, saving model to /content/best_model.hdf5\n",
            "Epoch 115/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.4555 - acc: 0.8423 - val_loss: 0.5936 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.84719\n",
            "Epoch 116/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4523 - acc: 0.8418 - val_loss: 0.6139 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.84719\n",
            "Epoch 117/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4425 - acc: 0.8443 - val_loss: 0.7194 - val_acc: 0.7948\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.84719\n",
            "Epoch 118/150\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.4401 - acc: 0.8435 - val_loss: 0.8513 - val_acc: 0.7782\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.84719\n",
            "Epoch 119/150\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.4512 - acc: 0.8423 - val_loss: 0.4663 - val_acc: 0.8539\n",
            "\n",
            "Epoch 00119: val_acc improved from 0.84719 to 0.85395, saving model to /content/best_model.hdf5\n",
            "Epoch 118: Reached baseline, terminating training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdd22ff88d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}